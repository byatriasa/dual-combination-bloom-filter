# -*- coding: utf-8 -*-
"""DCOMB

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CikBboWfu-cWmu3KGrErLpJQPBOvWxia

Library
"""

import math
import time
import hashlib
import pandas as pd

from bitarray import bitarray
from google.colab import files

"""# CONFIG: Set constants' value"""

# static bloom filter capacity
n = 20

 # default false positive probablity
p = 0.001

 # pk code group size in ubf
a = 1000

# set the number of block that will be used
block_count = 1000

"""# DATASET: Load from CSV
Load dataset from prepared csv files.
"""

## Read csv dataset
source_block_df = pd.read_csv('https://gist.githubusercontent.com/alanmsmxyz/eb7e0007215c3d46e8aa57e72c609ace/raw/19ff43c52c6fd8270e8c70a0bc31f042ba19e352/bf_block.csv')
source_index_df = pd.read_csv('https://gist.githubusercontent.com/alanmsmxyz/eb7e0007215c3d46e8aa57e72c609ace/raw/19ff43c52c6fd8270e8c70a0bc31f042ba19e352/bf_index.csv')
keys_df = pd.read_csv('https://gist.githubusercontent.com/alanmsmxyz/eb7e0007215c3d46e8aa57e72c609ace/raw/19ff43c52c6fd8270e8c70a0bc31f042ba19e352/bf_keys.csv')

# filter n amount of block to be used
block_df = source_block_df.head(block_count)

# get all index for filtered blocks
latest_block = block_df['block_id'][len(block_df) - 1]
index_df = source_index_df.loc[source_index_df['block_id'] <= latest_block]

print(index_df)

"""# CLASS: Bloom Filter

Class: __Bloom Filter__
> Consist of:
- Function: __init__: to initialize the class, functions, and class methods functions used
- Function: __add__: to insert a specified desired element to the array
- Function: __check__: to an existence of a specified element in the array
- Function: (classmethod) __get_size__: to calculate the size of the array
- Function: (classmethod) __get_hash_count__: to calculate the hash count needed to hash a specified element
"""

class BloomFilter(object):

	'''
	Class for Bloom filter, using SHA256 hash function
	'''

	def __init__(self, items_count, fp_prob):
		'''
    Bloom Filter is a bit of array of specified size (m) and initially sets to zero

    Glosarium:
		  n = items_count : int
			  Number of items expected to be stored in bloom filter
		  p = fp_prob : float
			  False Positive probability in decimal
      k = hash count
        Hash count needed for specified value. Formula commented alongside the function.
      m = size of array
        m CAN'T BE INPUTED MANUALLY without calculating the items count and hash count. Otherwise, collision increases.
		'''

		# Initialize false positive probability in decimal
		self.fp_prob = fp_prob

		# Initialize size (m) of bit array to use
		self.size = self.get_size(items_count, fp_prob)

		# Initialize number of hash functions (k) to use
		self.hash_count = self.get_hash_count(self.size, items_count)

		# Initialize bit array of given size
    # Creating the array that will use the bloom filter method
		self.bit_array = bitarray(self.size)

		# Initialize all bits as 0
		self.bit_array.setall(0)

	@classmethod
	def from_bit_array(self, text, n, fp_prob):
		ba = bitarray(len(text))

		for i in range(len(text)):
			ba[i] = int(text[i])

		bf = BloomFilter(n, fp_prob)

		bf.bit_array = ba

		return bf

	def add(self, item):
		'''
		Encode and insert an item into the filter
		'''
		digests = []
		for i in range(self.hash_count):

			# create digest for given item.
			# using SHA256
      # checking the bit value
      # set bit value = position mod m
			digest = int(hashlib.sha256(item.encode()).hexdigest(),16) % self.size
			digests.append(digest)

			# set the bit True in bit_array
			self.bit_array[digest] = True

	def check(self, item):
		'''
		Check for existence of an item in filter
		'''
		for i in range(self.hash_count):
			digest = int(hashlib.sha256(item.encode()).hexdigest(),16) % self.size
			if self.bit_array[digest] == False:

				# if any of bit is False then,its not present
				# in filter
				# else there is probability that it exist
				return False
		return True

	@classmethod
	def get_size(self, n, p):
		'''
		Return the size of bit array(m) to used using
		following formula
		m = -(n * lg(p)) / (lg(2)^2)
		n : int
			number of items expected to be stored in filter
		p : float
			False Positive probability in decimal
		'''
		m = -(n * math.log(p))/(math.log(2)**2)
		return int(m)

	@classmethod
	def get_hash_count(self, m, n):
		'''
		Return the hash function(k) to be used using
		following formula
		k = (m/n) * lg(2)

		m : int
			size of bit array
		n : int
			number of items expected to be stored in filter
		'''
		k = (m/n) * math.log(2)
		return int(k)

"""# FUNCTION: create_list

"""

def create_list(array_count, element_count, fp_prob):
  '''
  Function to create multiple array that recall bloom filter method.
  '''
  array_bf = []
  a = array_count
  n = element_count
  p = fp_prob

  for i in range(a):
    BF = BloomFilter(n, p)
    array_bf.append(BF)

  return array_bf

  # baca data dari csv, insert.

"""# DCOMB: Layer 1

## Static
"""

def create_layer_1_static(layer_1_static_list):
  layer_1_static_list.append(BloomFilter(n, p))

  return layer_1_static_list

def add_layer_1_static(layer_1_static_list, block_id, block_indexes_df):
  layer_1_static_list

  for index in block_indexes_df['index']:
    layer_1_static_list[block_id].add(index)

  return layer_1_static_list

def query_layer_1_static(layer_1_static_list, block_id, index):
  return layer_1_static_list[block_id].check(index)

"""## Dynamic"""

def create_layer_1_dynamic(layer_1_dynamic_list, block_indexes_df):
  layer_1_dynamic_list.append(BloomFilter(len(block_indexes_df.index), p))

  return layer_1_dynamic_list

def add_layer_1_dynamic(layer_1_dynamic_list, block_id, block_indexes_df):
  for index in block_indexes_df['index']:
    layer_1_dynamic_list[block_id].add(index)

  return layer_1_dynamic_list

def query_layer_1_dynamic(layer_1_dynamic_list, block_id, index):
  return layer_1_dynamic_list[block_id].check(index)

"""# DCOMB: Layer 2"""

def get_pk_from_index(index):
  # index consist of
  # stream head hash (40)+ cipher hash (40) + pk
  return index[80:]

def pk_to_pk_code(pk):
  spec = '{fill}{align}{width}{type}'.format(fill='0', align='>', width=len(pk) * 4, type='b')

  return format(int(pk, 16), spec)

def get_longest_pk(block_index_df):
  longest_pk = ''

  for index in block_index_df['index']:
    current_index_pk = get_pk_from_index(index)

    if len(current_index_pk) > len(longest_pk):
      longest_pk = current_index_pk

  return longest_pk

def get_padded_pk_code_list(block_index_df, block_longest_pk_code):
  block_pk_code_length = len(block_longest_pk_code)
  block_pk_code_list = []

  for index in block_index_df['index']:
    # padd 0 (ljust) of each index pk until len(pk_code) == len(block_longest_pk_code)
    pk_code = pk_to_pk_code(get_pk_from_index(index))
    pk_code = pk_code.ljust(block_pk_code_length, '0')

    block_pk_code_list.append(pk_code)

  return block_pk_code_list

"""## 2.1. UBF1"""

def create_ubf1(ubf1_list, block_id, block_longest_pk_code):
  f = len(block_longest_pk_code)
  union_bits_count = f - a + 1

  ubf1 = create_list(union_bits_count, n, p)

  ubf1_list.append(ubf1)
  
  return ubf1_list

def add_ubf1(ubf1_list, block_id, block_longest_pk_code, block_pk_code_list):
  for pk_code in block_pk_code_list:    

    for u in range(len(ubf1_list[block_id])):
      ubf1_list[block_id][u].add(pk_code[:a])

      pk_code = pk_code[1:]
    
  return ubf1_list

def query_ubf1(ubf1_list, block_id, pk_code):
  query_result = True

  for u in range(len(ubf1_list[block_id])):
    query_result = query_result and ubf1_list[block_id][u].check(pk_code[:a])

    pk_code = pk_code[1:]

  return query_result

"""## 2.1. UBF2"""

def create_ubf2(ubf2_list, block_id, block_longest_pk_code):
  f = len(block_longest_pk_code)
  union_bits_count = -(f // -a) # equal to ceil(f / a)

  ubf2 = create_list(union_bits_count, n, p)

  ubf2_list.append(ubf2)
  
  return ubf2_list

def add_ubf2(ubf2_list, block_id, block_longest_pk_code, block_pk_code_list):  
  for pk_code in block_pk_code_list:
    temp = pk_code 

    for u in range(len(ubf2_list[block_id])):
      ubf2_list[block_id][u].add(pk_code[:a])

      pk_code = pk_code[a:]

  return ubf2_list

def query_ubf2(ubf2_list, block_id, pk_code):
  query_result = True

  for u in range(len(ubf2_list[block_id])):
    query_result = query_result and ubf2_list[block_id][u].check(pk_code[:a])

    pk_code = pk_code[a:]

  return query_result

"""## 2.2. DBF"""

def create_dbf(dbf_list, block_id, block_longest_pk_code):
  dbf = create_list(len(block_longest_pk_code), n, p)

  dbf_list.append(dbf)
  
  return dbf_list

def add_dbf(dbf_list, block_id, block_longest_pk_code, block_indexes_df):  
  for i in range(len(block_longest_pk_code)):
    if block_longest_pk_code[i] == '0': continue
    
    for index in block_indexes_df['index']:
      dbf_list[block_id][i].add(index)

  return dbf_list

def query_dbf(dbf_list, block_id, longest_pk_code, index):  
  query_result = True

  for i in range(len(block_longest_pk_code)):
    if block_longest_pk_code[i] == '0': continue
    
    query_result = query_result and dbf_list[block_id][i].check(index)

  return query_result

"""# GLOBAL VARIABLES
To store generated dcomb result
"""

layer_1_static_list = []
layer_1_dynamic_list = []
layer_2_ubf1_list = []
layer_2_ubf2_list = []
layer_2_dbf_list = []

"""# BENCHMARK: Insert
Insert is done per block basis
"""

insert_time_df = pd.DataFrame(data = {
    'block_id': [],
    'layer_1_static': [],
    'layer_1_dynamic': [],
    'layer_2_ubf1': [],
    'layer_2_ubf2': [],
    'layer_2_dbf': [],
    'preparation': [], # time used to grab pk code from index, convert it to pk code
})

# iterate through every block available in prepared data
for block_id in block_df['block_id']:
  # get all indexes for corresponding block
  block_indexes_df = index_df.loc[index_df['block_id'] == block_id]

  # preparation
  t0_prep = time.perf_counter()

  block_longest_pk = get_longest_pk(block_indexes_df)
  block_longest_pk_code = pk_to_pk_code(block_longest_pk)

  pk_code_list = get_padded_pk_code_list(block_indexes_df, block_longest_pk_code)

  t1_prep = time.perf_counter()
  tn_prep = t1_prep - t0_prep
  # preparation



  # insert to layer 1 static
  t0_l1s = time.perf_counter()

  layer_1_static_list = create_layer_1_static(layer_1_static_list)
  layer_1_static_list = add_layer_1_static(layer_1_static_list, block_id, block_indexes_df)

  t1_l1s = time.perf_counter()
  tn_l1s = t1_l1s - t0_l1s
  # endof insert to layer 1 static



  # insert to layer 1 dynamic
  t0_l1d = time.perf_counter()

  layer_1_dynamic_list = create_layer_1_dynamic(layer_1_dynamic_list, block_indexes_df)
  layer_1_dynamic_list = add_layer_1_dynamic(layer_1_dynamic_list, block_id, block_indexes_df)

  t1_l1d = time.perf_counter()
  tn_l1d = t1_l1d - t0_l1d
  # endof insert to layer 1 dynamic



  # insert to layer 2 ubf1
  t0_ubf1 = time.perf_counter()

  layer_2_ubf1_list = create_ubf1(layer_2_ubf1_list, block_id, block_longest_pk_code)
  layer_2_ubf1_list = add_ubf1(layer_2_ubf1_list, block_id, block_longest_pk_code, pk_code_list)

  t1_ubf1 = time.perf_counter()
  tn_ubf1 = t1_ubf1 - t0_ubf1
  # endof insert to layer 2 ubf1


  # insert to layer 2 ubf2
  t0_ubf2 = time.perf_counter()

  layer_2_ubf2_list = create_ubf2(layer_2_ubf2_list, block_id, block_longest_pk_code)
  layer_2_ubf2_list = add_ubf2(layer_2_ubf2_list, block_id, block_longest_pk_code, pk_code_list)

  t1_ubf2 = time.perf_counter()
  tn_ubf2 = t1_ubf2 - t0_ubf2
  # endof insert to layer 2 ubf2


  # insert to layer 2 dbf
  t0_dbf = time.perf_counter()

  layer_2_dbf_list = create_dbf(layer_2_dbf_list, block_id, block_longest_pk_code)
  layer_2_dbf_list = add_dbf(layer_2_dbf_list, block_id, block_longest_pk_code, block_indexes_df)

  t1_dbf = time.perf_counter()
  tn_dbf = t1_dbf - t0_dbf
  # endof insert to layer 2 dbf


  insert_time_df = insert_time_df.append({
      'block_id': block_id,
      'layer_1_static': tn_l1s,
      'layer_1_dynamic': tn_l1d,
      'layer_2_ubf1': tn_ubf1,
      'layer_2_ubf2': tn_ubf2,
      'layer_2_dbf': tn_dbf,
      'preparation': tn_prep,
  }, ignore_index=True)

insert_time_df['block_id'] = insert_time_df['block_id'].apply(int)

# export to csv
insert_time_df.to_csv('insert_time_df.csv', index=False)
files.download('insert_time_df.csv')

print(insert_time_df)

"""# BENCHMARK: Query"""

query_time_df = pd.DataFrame(data = {
    'block_id': [],
    'index': [],
    'result_layer_1_static': [],
    'time_layer_1_static': [],
    'result_layer_1_dynamic': [],
    'time_layer_1_dynamic': [],
    'result_layer_2_ubf1': [],
    'time_layer_2_ubf1': [],
    'result_layer_2_ubf2': [],
    'time_layer_2_ubf2': [],
    'result_layer_2_dbf': [],
    'time_layer_2_dbf': [],
    'preparation': [],
})

# iterate through every block available in prepared data
for block_id in block_df['block_id']:
  block_indexes_df = index_df.loc[index_df['block_id'] == block_id]

  block_longest_pk = get_longest_pk(block_indexes_df)
  block_longest_pk_code = pk_to_pk_code(block_longest_pk)

  for index in block_indexes_df['index']:
    # # preparation
    t0_prep = time.perf_counter()

    pk = get_pk_from_index(index)
    pk_code = pk_to_pk_code(pk)
    pk_code = pk_code.ljust(len(block_longest_pk_code), '0')

    t1_prep = time.perf_counter()
    tn_prep = t1_prep - t0_prep
    # # preparation


    # # query to layer 1 static
    t0_l1s = time.perf_counter()

    r_l1s = layer_1_static_list[block_id].check(index)

    t1_l1s = time.perf_counter()
    tn_l1s = t1_l1s - t0_l1s
    # # endof query to layer 1 static



    # # query to layer 1 dynamic
    t0_l1d = time.perf_counter()

    r_l1d = layer_1_dynamic_list[block_id].check(index)

    t1_l1d = time.perf_counter()
    tn_l1d = t1_l1d - t0_l1d
    # # endof query to layer 1 dynamic



    # # query to layer 2 ubf1
    t0_ubf1 = time.perf_counter()

    r_ubf1 = query_ubf1(layer_2_ubf1_list, block_id, pk_code)

    t1_ubf1 = time.perf_counter()
    tn_ubf1 = t1_ubf1 - t0_ubf1
    # # endof query to layer 2 ubf1


    # # query to layer 2 ubf2
    t0_ubf2 = time.perf_counter()

    r_ubf2 = query_ubf2(layer_2_ubf2_list, block_id, pk_code)

    t1_ubf2 = time.perf_counter()
    tn_ubf2 = t1_ubf2 - t0_ubf2
    # # endof query to layer 2 ubf2


    # # insert to layer 2 dbf
    t0_dbf = time.perf_counter()

    r_dbf = query_dbf(layer_2_dbf_list, block_id, block_longest_pk_code, index)

    t1_dbf = time.perf_counter()
    tn_dbf = t1_dbf - t0_dbf
    # # endof insert to layer 2 dbf


    query_time_df = query_time_df.append({
        'block_id': block_id,
        'index': index,
        'result_layer_1_static': r_l1s,
        'time_layer_1_static': tn_l1s,
        'result_layer_1_dynamic': r_l1d,
        'time_layer_1_dynamic': tn_l1d,
        'result_layer_2_ubf1': r_ubf1,
        'time_layer_2_ubf1': tn_ubf1,
        'result_layer_2_ubf2': r_ubf2,
        'time_layer_2_ubf2': tn_ubf2,
        'result_layer_2_dbf': r_dbf,
        'time_layer_2_dbf': tn_dbf,

        'preparation': tn_prep,
    }, ignore_index=True)

query_time_df['block_id'] = query_time_df['block_id'].apply(int)
query_time_df['result_layer_1_static'] = query_time_df['result_layer_1_static'].apply(bool)
query_time_df['result_layer_1_dynamic'] = query_time_df['result_layer_1_dynamic'].apply(bool)
query_time_df['result_layer_2_ubf1'] = query_time_df['result_layer_2_ubf1'].apply(bool)
query_time_df['result_layer_2_ubf2'] = query_time_df['result_layer_2_ubf2'].apply(bool)
query_time_df['result_layer_2_dbf'] = query_time_df['result_layer_2_dbf'].apply(bool)

# export to csv
query_time_df.to_csv('query_time_df.csv', index=False)
files.download('query_time_df.csv')

print(query_time_df)

"""# BENCHMARK: FPP"""

fpp_df = pd.DataFrame(data = {
    'block_id': [],
    'layer_1_static': [],
    'layer_1_dynamic': [],
    'layer_2_ubf1': [],
    'layer_2_ubf2': [],
    'layer_2_dbf': [],
})

for block_id in block_df['block_id']:

  # calculate fpp for layer 1 static
  l1s_n = n # n for layer 1 static
  l1s_k = layer_1_static_list[block_id].hash_count # k for layer 1 static

  l1s_fpp = pow(1 - (pow(1 - (1 / 383), (l1s_n * l1s_k))), l1s_k)
  # endof calculate fpp for layer 1 static



  # calculate fpp for layer 1 dynamic
  l1d_n = len(index_df.loc[index_df['block_id'] == block_id].index) # n for layer 1 dynamic
  l1d_k = layer_1_dynamic_list[block_id].hash_count # k layer 1 dynamic

  l1d_fpp = pow(1 - (pow(1 - (1 / 383), (l1d_n * l1d_k))), l1d_k)
  # endof calculate fpp for layer 1 dynamic



  block_indexes_df = index_df.loc[index_df['block_id'] == block_id]
  block_longest_pk = get_longest_pk(block_indexes_df)
  block_longest_pk_code = pk_to_pk_code(block_longest_pk)

  f = len(block_longest_pk_code) # f for ubf1, ubf2, and dbf



  # calculate fpp for layer 2 ubf1

  # get constant from ubf_id 0 for the block
  # since all bf in same block are having the same properties
  ubf1_k = layer_2_ubf1_list[block_id][0].hash_count # k for ubf1
  ubf1_m = layer_2_ubf1_list[block_id][0].size # m for ubf1

  ubf1_fpp1 = (f - a + 2)
  ubf1_fpp2 = pow(1 - pow(math.e, -1 * ((n * ubf1_k) / ubf1_m)), ubf1_k)
  ubf1_fpp = ubf1_fpp1 * ubf1_fpp2 
  # endof calculate fpp for layer 2 ubf1



  # calculate fpp for layer 2 ubf2
  
  # get constant from ubf_id 0 for the block
  # since all bf in same block are having the same properties
  ubf2_k = layer_2_ubf2_list[block_id][0].hash_count # k for ubf2
  ubf2_m = layer_2_ubf2_list[block_id][0].size # m for ubf2

  ubf2_fpp1 = -(f // -a) + 1
  ubf2_fpp2 = pow(1 - pow(math.e, -1 * ((n * ubf2_k) / ubf2_m)), ubf2_k)
  ubf2_fpp = ubf2_fpp1 * ubf2_fpp2 
  # endof calculate fpp for layer 2 ubf2


  # calculate fpp for layer 2 dbf
  dbf_q = a

  dbf_fpp1 = pow(p, dbf_q)
  dbf_fpp2 = pow(1 - p, f - dbf_q + 2)
  dbf_fpp = dbf_fpp1 * dbf_fpp2 
  # endof calculate fpp for layer 2 dbf

  fpp_df = fpp_df.append({
    'block_id': block_id,
    'layer_1_static': l1s_fpp,
    'layer_1_dynamic': l1d_fpp,
    'layer_2_ubf1': ubf1_fpp,
    'layer_2_ubf2': ubf2_fpp,
    'layer_2_dbf': dbf_fpp,
  }, ignore_index=True)

fpp_df['block_id'] = fpp_df['block_id'].apply(int)

# export to csv
fpp_df.to_csv('fpp_df.csv', index=False)
files.download('fpp_df.csv')

print(fpp_df)

"""# BENCHMARK: Storage"""

layer_1_static_size = 0
for block_bf in layer_1_static_list:
  layer_1_static_size += block_bf.size

print('layer 1 static:', layer_1_static_size, 'bits')

layer_1_dynamic_size = 0
for block_bf in layer_1_dynamic_list:
  layer_1_dynamic_size += block_bf.size

print('layer 1 dynamic:', layer_1_dynamic_size, 'bits')

layer_2_ubf1_size = 0
for block in layer_2_ubf1_list:
  for bf in block:
    layer_2_ubf1_size += bf.size

print('layer 2 ubf1:', layer_2_ubf1_size, 'bits')

layer_2_ubf2_size = 0
for block in layer_2_ubf2_list:
  for bf in block:
    layer_2_ubf2_size += bf.size

print('layer 2 ubf2:', layer_2_ubf2_size, 'bits')

layer_2_dbf_size = 0
for block in layer_2_dbf_list:
  for bf in block:
    layer_2_dbf_size += bf.size

print('layer 2 dbf:', layer_2_dbf_size, 'bits')

print('layer 1 static:', layer_1_static_size, 'bits')
print('layer 1 dynamic:', layer_1_dynamic_size, 'bits')
print('layer 2 ubf1:', layer_2_ubf1_size, 'bits')
print('layer 2 ubf2:', layer_2_ubf2_size, 'bits')
print('layer 2 dbf:', layer_2_dbf_size, 'bits')

"""# EXPORT: CSV"""

layer_1_static_df = pd.DataFrame(data = {
    'block_id': [],
    'bit_array': [],
})


for i in range(len(layer_1_static_list)):
  bit_array = ''.join(str(x) for x in layer_1_static_list[i].bit_array)
  layer_1_static_df = layer_1_static_df.append({
      'block_id': i,
      'bit_array': bit_array
  }, ignore_index=True)


layer_1_static_df['block_id'] = layer_1_static_df['block_id'].apply(int)

# print(layer_1_static_df)

layer_1_static_df.to_csv('layer_1_static_df.csv', index=False)
files.download('layer_1_static_df.csv')

layer_1_dynamic_df = pd.DataFrame(data = {
    'block_id': [],
    'bit_array': [],
})

for i in range(len(layer_1_dynamic_list)):
  bit_array = ''.join(str(x) for x in layer_1_dynamic_list[i].bit_array)
  layer_1_dynamic_df = layer_1_dynamic_df.append({
      'block_id': i,
      'bit_array': bit_array
  }, ignore_index=True)


layer_1_dynamic_df['block_id'] = layer_1_dynamic_df['block_id'].apply(int)

# print(layer_1_dynamic_df)

layer_1_dynamic_df.to_csv('layer_1_dynamic_df.csv', index=False)
files.download('layer_1_dynamic_df.csv')

layer_2_ubf1_df = pd.DataFrame(data = {
    'block_id': [],
    'ubf_id': [],
    'bit_array': [],
})

for i in range(len(layer_2_ubf1_list)):
  for j in range(len(layer_2_ubf1_list[i])):
    bit_array = ''.join(str(x) for x in layer_2_ubf1_list[i][j].bit_array)

    layer_2_ubf1_df = layer_2_ubf1_df.append({
        'block_id': i,
        'ubf_id': j,
        'bit_array': bit_array
    }, ignore_index=True)
  
  
layer_2_ubf1_df['block_id'] = layer_2_ubf1_df['block_id'].apply(int)
layer_2_ubf1_df['ubf_id'] = layer_2_ubf1_df['ubf_id'].apply(int)

layer_2_ubf1_df.to_csv('layer_2_ubf1_df.csv', index=False)
files.download('layer_2_ubf1_df.csv')

layer_2_ubf2_df = pd.DataFrame(data = {
    'block_id': [],
    'ubf_id': [],
    'bit_array': [],
})

for i in range(len(layer_2_ubf2_list)):
  for j in range(len(layer_2_ubf2_list[i])):
    bit_array = ''.join(str(x) for x in layer_2_ubf2_list[i][j].bit_array)

    layer_2_ubf2_df = layer_2_ubf2_df.append({
        'block_id': i,
        'ubf_id': j,
        'bit_array': bit_array
    }, ignore_index=True)
  
  
layer_2_ubf2_df['block_id'] = layer_2_ubf2_df['block_id'].apply(int)
layer_2_ubf2_df['ubf_id'] = layer_2_ubf2_df['ubf_id'].apply(int)

layer_2_ubf2_df.to_csv('layer_2_ubf2_df.csv', index=False)
files.download('layer_2_ubf2_df.csv')

"""**IMPORTANT NOTE!**

**Exporting dbf to csv might cause runtime to crash (out of memory)**

**Since each block having ~1000 bloom filters**
"""

layer_2_dbf_df = pd.DataFrame(data = {
    'block_id': [],
    'dbf_id': [],
    'bit_array': [],
})

for i in range(len(layer_2_dbf_list)):
  for j in range(len(layer_2_dbf_list[i])):
    bit_array = ''.join(str(x) for x in layer_2_dbf_list[i][j].bit_array)

    layer_2_dbf_df = layer_2_dbf_df.append({
        'block_id': i,
        'dbf_id': j,
        'bit_array': bit_array
    }, ignore_index=True)

layer_2_dbf_df['block_id'] = layer_2_dbf_df['block_id'].apply(int)
layer_2_dbf_df['dbf_id'] = layer_2_dbf_df['dbf_id'].apply(int)

layer_2_dbf_df.to_csv('layer_2_dbf_df.csv', index=False)
files.download('layer_2_dbf_df.csv')